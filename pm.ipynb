{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPyk7Ij9Nc7mVJzw8oySx4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lingche1/msc1/blob/main/pm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "853c677f"
      },
      "source": [
        "# Task\n",
        "Fine-tune the \"Weyaxi/Einstein-v6.1-Llama3-8B\" model using LoRA on a small, 429-entry English AIpaca dataset to adapt it for the quantum materials domain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cff087d"
      },
      "source": [
        "## Install necessary libraries\n",
        "\n",
        "### Subtask:\n",
        "Install `transformers`, `datasets`, `peft`, `trl`, `bitsandbytes`, `accelerate`, and `scipy`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d5bed1e"
      },
      "source": [
        "**Reasoning**:\n",
        "Install the necessary libraries using pip.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ad88c41b",
        "outputId": "583c0fe1-2548-4d5f-e7ec-c49a04acadb0"
      },
      "source": [
        "%pip install transformers datasets peft trl accelerate scipy\n",
        "%pip install -U bitsandbytes"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/main.py\", line 78, in main\n",
            "    command = create_command(cmd_name, isolated=(\"--isolated\" in cmd_args))\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/__init__.py\", line 114, in create_command\n",
            "    module = importlib.import_module(module_path)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 15, in <module>\n",
            "    from pip._internal.cli.req_command import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/req_command.py\", line 18, in <module>\n",
            "    from pip._internal.index.collector import LinkCollector\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/index/collector.py\", line 31, in <module>\n",
            "    from pip._vendor import requests\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/requests/__init__.py\", line 159, in <module>\n",
            "    from .api import delete, get, head, options, patch, post, put, request\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/requests/api.py\", line 11, in <module>\n",
            "    from . import sessions\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/requests/sessions.py\", line 15, in <module>\n",
            "    from .adapters import HTTPAdapter\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/requests/adapters.py\", line 47, in <module>\n",
            "    from .models import Response\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/requests/models.py\", line 54, in <module>\n",
            "    from .status_codes import codes\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 936, in exec_module\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1032, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1131, in get_data\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-2737251016.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'install transformers datasets peft trl accelerate scipy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'install -U bitsandbytes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2416\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2418\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2419\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_installation_commands.py\u001b[0m in \u001b[0;36m_pip_magic\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0;31m# Colab is set up such that pip does the right thing, and pip install\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0;31m# will properly trigger the pip install warning.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m       \u001b[0m_pip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_send_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36mprint_previous_import_warning\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;34m\"\"\"Prints a warning about previously imported packages.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mpackages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpackages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# display a list of packages using the colab-display-data mimetype, which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_previously_imported_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;34m\"\"\"List all previously imported packages from a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0minstalled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_toplevel_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstalled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_extract_toplevel_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;34m\"\"\"Extract the list of toplevel packages associated with a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mtoplevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages_distributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mtoplevel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mpackages_distributions\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1073\u001b[0m     \u001b[0mpkg_to_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1075\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_top_level_declared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_top_level_inferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1076\u001b[0m             \u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m_top_level_inferred\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m   1085\u001b[0m     return {\n\u001b[1;32m   1086\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malways_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1088\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuffix\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\".py\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m     }\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mfiles\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         return skip_missing_files(\n\u001b[0m\u001b[1;32m    605\u001b[0m             make_files(\n\u001b[1;32m    606\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_files_distinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/_functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(param, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mskip_missing_files\u001b[0;34m(package_paths)\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1233\u001b[0m         \"\"\"\n\u001b[1;32m   1234\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ignore_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36mstat\u001b[0;34m(self, follow_symlinks)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mdoes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \"\"\"\n\u001b[0;32m-> 1013\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mowner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a528b811"
      },
      "source": [
        "## Load the model and tokenizer\n",
        "\n",
        "### Subtask:\n",
        "Load the `Weyaxi/Einstein-v6.1-Llama3-8B` model and its corresponding tokenizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00018e24"
      },
      "source": [
        "**Reasoning**:\n",
        "Import necessary classes and load the tokenizer and model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de7fb238"
      },
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "model_name = \"Weyaxi/Einstein-v6.1-Llama3-8B\"\n",
        "\n",
        "# Load model in 4-bit precision for Q-LoRA\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9f7261e"
      },
      "source": [
        "## Load and preprocess the dataset\n",
        "\n",
        "### Subtask:\n",
        "Load the small AIpaca dataset and preprocess it for fine-tuning. This may involve formatting the data into the correct input format for the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQQmxSKWIKbd"
      },
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"json\", data_files=\"/content/alpaca-cleaned.json\")\n",
        "\n",
        "# Select the first 429 entries\n",
        "small_dataset = dataset[\"train\"].select(range(429))\n",
        "\n",
        "# Define the preprocessing function\n",
        "def preprocess_function(examples):\n",
        "    texts = []\n",
        "    for instruction, input_text, output_text in zip(examples['instruction'], examples['input'], examples['output']):\n",
        "        if input_text:\n",
        "            text = f\"Instruction: {instruction}\\nInput: {input_text}\\nOutput: {output_text}\"\n",
        "        else:\n",
        "            text = f\"Instruction: {instruction}\\nOutput: {output_text}\"\n",
        "        texts.append(text)\n",
        "    tokenized_inputs = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=512)\n",
        "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy() # Create labels for causal language modeling\n",
        "    return tokenized_inputs\n",
        "\n",
        "# Apply the preprocessing function\n",
        "processed_dataset = small_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Remove original text columns\n",
        "processed_dataset = processed_dataset.remove_columns(['instruction', 'input', 'output', 'system'])\n",
        "\n",
        "# Set the format to torch\n",
        "processed_dataset.set_format(type='torch')\n",
        "\n",
        "display(processed_dataset)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "\n",
        "# Convert the processed_dataset to a pandas DataFrame for splitting\n",
        "processed_df = processed_dataset.to_pandas()\n",
        "\n",
        "# Split the DataFrame\n",
        "train_df, val_df = train_test_split(processed_df, test_size=0.15, random_state=42)\n",
        "\n",
        "# Convert the DataFrames back to Dataset objects\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "\n",
        "display(train_dataset)\n",
        "display(val_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61abcda3"
      },
      "source": [
        "## Configure lora\n",
        "\n",
        "### Subtask:\n",
        "Set up the LoRA configuration for efficient fine-tuning. This involves specifying the LoRA parameters like `r`, `lora_alpha`, and `lora_dropout`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3e1f39c"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the LoraConfig class and instantiate it with the specified parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff2f8aed"
      },
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=64, # Increased r for potentially better performance with Q-LoRA\n",
        "    lora_alpha=16, # Adjusted alpha based on r\n",
        "    lora_dropout=0.1, # Increased dropout for regularization\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "print(lora_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2a4edc9"
      },
      "source": [
        "## Configure training parameters\n",
        "\n",
        "### Subtask:\n",
        "Set up the training arguments, including hyperparameters like learning rate, batch size, number of epochs, and optimization strategy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab829516"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the `TrainingArguments` class and instantiate it with the specified parameters. Then print the object to verify the configuration.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c52a7511"
      },
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    learning_rate=2e-5,\n",
        "    optim=\"adamw_torch\",\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "print(training_args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20ae8fd9"
      },
      "source": [
        "## Prepare the model for lora training\n",
        "\n",
        "### Subtask:\n",
        "Wrap the base model with the LoRA layers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08354ce6"
      },
      "source": [
        "**Reasoning**:\n",
        "Wrap the base model with the LoRA layers and print the number of trainable parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "862639f0"
      },
      "source": [
        "from peft import get_peft_model\n",
        "\n",
        "model.enable_input_require_grads()\n",
        "model.config.use_cache = False\n",
        "model = get_peft_model(\n",
        "    model,\n",
        "    lora_config,\n",
        "\n",
        ")\n",
        "\n",
        "# 2) 确保在 GPU 上\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "print(peft_model.print_trainable_parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e95fc84d"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "**Can the \"Weyaxi/Einstein-v6.1-Llama3-8B\" model be fine-tuned on a small AIpaca dataset with LoRA given the available computational resources?**\n",
        "\n",
        "No, the \"Weyaxi/Einstein-v6.1-Llama3-8B\" model could not be fine-tuned on the provided dataset with the available computational resources. The training process repeatedly failed due to `OutOfMemoryError` on the GPU, even after implementing several memory-saving techniques.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "* The training process failed with an `OutOfMemoryError`, indicating that the \"Weyaxi/Einstein-v6.1-Llama3-8B\" model is too large for the available GPU memory (approximately 40GB).\n",
        "* Several memory optimization techniques were attempted, including:\n",
        "  * Reducing the `per_device_train_batch_size` to 1.\n",
        "  * Enabling `gradient_checkpointing`.\n",
        "  * Reducing the `max_length` of the tokenized sequences to 256.\n",
        "* Despite these optimizations, the `OutOfMemoryError` persisted, suggesting that the memory issue is not solvable with these techniques alone on the given hardware.\n",
        "\n",
        "### Insights or Next Steps\n",
        "* To successfully fine-tune this model, it is necessary to use a more powerful GPU with a larger memory capacity.\n",
        "* An alternative approach could be to use a smaller, less memory-intensive base model for fine-tuning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb8da20f"
      },
      "source": [
        "## Set up and run the SFTTrainer\n",
        "\n",
        "### Subtask:\n",
        "Set up the `SFTTrainer` and start the training process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "686040b7"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the `SFTTrainer` class and instantiate it with the configured model, dataset, tokenizer, and training arguments. Then, call the `train()` method to start the fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "debafc27"
      },
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=peft_model,\n",
        "    train_dataset=processed_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e15fe42"
      },
      "source": [
        "## Save the fine-tuned model\n",
        "\n",
        "### Subtask:\n",
        "Save the fine-tuned model to a specified directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "482d3eb4"
      },
      "source": [
        "**Reasoning**:\n",
        "Save the fine-tuned model using the `save_model` method of the trainer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3bd2c41"
      },
      "source": [
        "# Save the fine-tuned model\n",
        "trainer.save_model(\"./fine_tuned_model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22d9acac"
      },
      "source": [
        "## Load and evaluate the fine-tuned model\n",
        "\n",
        "### Subtask:\n",
        "Load the fine-tuned model and perform manual evaluation by generating text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4f7653d"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the base model and the fine-tuned LoRA adapters. Merge the adapters with the base model and then use the merged model to generate text for manual evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a7da305"
      },
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load the base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map={\"\": 0},\n",
        ")\n",
        "\n",
        "# Load the LoRA adapters\n",
        "model = PeftModel.from_pretrained(base_model, \"./fine_tuned_model\")\n",
        "\n",
        "# Merge the LoRA adapters with the base model\n",
        "merged_model = model.merge_and_unload()\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Function to generate text\n",
        "def generate_text(prompt, model, tokenizer, max_length=200):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(inputs[\"input_ids\"], max_length=max_length, num_return_sequences=1)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Example prompt for manual evaluation\n",
        "prompt = \"Instruction: What is the concept of a 'reciprocal lattice'and how is it related to the 'Brillouin zone' in solid-state physics?.\\nOutput:\"\n",
        "generated_text = generate_text(prompt, merged_model, tokenizer)\n",
        "\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}